# Cooperative Graph Colouring Problem

CGCP is a strategic game where rl players, either human or robot, collaborate to color a grid in such a way that neighboring blocks have distinct colors. Players interact with a grid containing 'b' blocks, where each block can be colored in one of 'c' colors. The game is repeated and stochastic.

- **Action**: (block,color)
- **State:** [(block,color),(block,color),(), ...]

## Game Configuration

- The grid's configuration is random yet fixed (not variable).
- Generated by the environment initially with a specific number of rows and columns (nxm).
- Certain cells are merged together, intended to have the same color from the start.
- We represent cells (merged or not) as blocks.
- Some blocks are hidden by the environment.
- Hidden blocks are symbolically grayed out.
- All blocks at the beginning of the game are hidden and colorless.

## Agents and Objectives

- Agents are q-learning agents, either human or robot (two types).
- Agents share a similar structure but differ in the payoff received.
- Payoff determined by factors assigning different weights to rewards based on player type.
- Action: assign color to block, e.g. (block, color).
- Objective: color the grid so that neighboring cells have different colors.
- Human and robot agents aim for correct color assignment with minimal cost (cooperative game).
- Correct color assignment is considered an equilibrium.
- Goal in other words, is to reach an equilibrium state.

## Game Structure

- The game is repeated (episodes) and consists of stages.
- Each stage consists of rounds, where agents act simultaneously.
- During each stage, the environment reveals a random bunch of blocks.
- Agents assign colors to these non-hidden blocks. This assignment occurs in rounds.
- When revealing hidden cells, the environment may assign colors to the cells with a probability 'p'.
- With a probability 'p-1', the environment keeps the revealed cells as they are (without color).
- Number of rounds is at least equal to: the number of the non-hidden cells in each stage / 2 (2-player game).

## Constraints and Rewards

- Constraint: neighboring cells cannot have the same color.
- Agents receive a penalty (-r) for conflicts violating the constraint and a reward (+r) for satisfying it.
- Additional reward (-x) added to total payoff as the average of certain factors.
- Each factor is assigned a value between 0 and 1 for both human and robot agents (e.g. h and r).
- Values are summed for each factor and processed to obtain the additional reward (-x): f(h,r) = -EXP(SUM(h,r)) = -x.

## Evaluation Algorithm

- The α-rank creates ranking sets from agents' meta-strategies (human, robot), e.g., {(h1, r1), (h3, r2), (h2, r1), ...}.
- Each agent (human or robot) is a trained RL model with separate parameters, distinguishing factors like model type or training duration.
- The game is a 2-player game with populations represented by 2 payoff tables (one for each player).
- Human players can play meta-strategies h1, h2, h3, ..., while robot players play corresponding r1, r2, r3, ..., etc.
- To assign values to payoff tables, run n simulations for each meta-strategy combination, e.g., n=100 simulations for (h1, r1), another 100 for (h1, r2), etc.
- Inferred "solutions" are derived from policies p*(s) = a formed by agents through training. Running simulations of agents (h3, r2), for example, reveals the most frequently occurring color assignment as the proposed "solution".

## Stochasticity ≡ Variability in Action Space

- Each round progresses until a terminal state (solution) is reached, similar to the structure of chess.
- In each stage, the environment "opens" certain blocks on the grid. Players execute actions of the form (block, color) without knowing if the block is open or closed. Actions are evaluated based on the rewards they receive.
- This means that the environment determines randomly which actions are accessible in each round, introducing variability and preventing a fixed set of actions across rounds.
- The stochasticity of the game is expressed through that variable action space.
- In other words, each action has a probability of failure p - making the game stochastic.

## Learning Algorithm

- Agents are trained with a learning algorithm aiming to learn an optimal policy p*(s) = a for each state s.
- The learning algorithm is likely to be DQL rather than TQL.

### ❌ Tabular Q-Learning (TBQ)

One major obstacle to implementing simple Q-learning agents in our game is the exponential growth of the state space. If you need to choose from c colors for each block b in the grid, then the total number of states (S) can be calculated as:

```python
S = c^b
```
where c represents the number of colors and b is the total number of blocks in the grid.

**Example:** For instance, if there are 6 colors (c = 6) and 9 blocks (b = 9), the total number of state (S) is 10,077,696. This immense state space does not allow us to train standard Q-learning agents for our game.

### ✔️ Deep Q-Learning (DQL)

One solution to the huge state space problem is to use function approximation methods like Deep Q-Networks (DQN). DQN employs neural networks to approximate the Q-values for state-action pairs. It takes the state as input and outputs Q-values for each possible action. This significantly reduces the need to explicitly store values for every possible state-action pair in a tabular form.

- **Input:** state
- **Output:** [p1, p2, ...], a list of values that indicates how good each action from that state is
- **Use:** for some state s, we use the nn once to output probabilities for all actions