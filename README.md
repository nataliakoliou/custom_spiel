# Cooperative Graph Colouring Problem

CGCP is a strategic game where q-learning agents, either human or robot, collaborate to color a grid in such a way that neighboring cells have distinct colors. The game involves stages, rounds, and hidden elements, challenging the agents to achieve equilibrium through color assignment while minimizing conflicts.

## Game Configuration

- The grid's configuration is random yet fixed (not variable).
- Generated by the environment initially with a specific number of rows and columns (nxm).
- Certain cells are merged together, intended to have the same color from the start.
- Some cells (merged or normal) are hidden by the environment.
- Hidden cells are symbolically grayed out.
- All cells at the beginning of the game are colorless.

## Agents and Objectives

- Agents are q-learning agents, either human or robot (two types).
- Agents share a similar structure but differ in the payoff received.
- Payoff determined by factors assigning different weights to rewards based on player type.
- Action: assign colour to cell, e.g. (colour, cell).
- Objective: color the grid so that neighboring cells have different colors.
- Human and robot agents aim for correct color assignment with minimal cost (cooperative game).
- Correct color assignment is considered an equilibrium.
- Goal in other words, is to reach an equilibrium state.

## Game Mechanics

- Game stages consist of rounds.
- Agents assign colors to all non-hidden cells in each stage.
- Environment randomly determines which agent plays in each round to prevent simultaneous coloring by multiple agents (priorities).
- Intermediate state intervenes between stages, during which the environment changes.
- After each stage, some cells are randomly hidden and previously hidden cells are revealed.
- Hidden and colored cells are revealed in subsequent stages.
- A threshold defines the maximum number of cells to hide in each intermediate state.
- When revealing hidden cells, the environment may change/assign colors to the cells with a probability 'p'.
- With a probability 'p-1', the environment maintains the revealed cells' colors as they are.
- Only during the intermediate state can the color of cells change due to external factors (environment), and not by the agents.
- Number of rounds is equal to the number of the non-hidden cells in each stage.
- This number varies over the stages of the game, due to environmental changes during the intermediate state.
- In each stage, agents observe how many non-hidden cells exist without a color assigned in the current stage.
- Throughout the stage, agents don't observe the colors assigned in the cells.
- Agents decide color assignment solely based on the q-values updated in the previous stage.
- Even though multiple actions are taken within a stage, its rounds are rather considered to occur simultaneously.

## Payoffs and Rewards

- Constraint: neighboring cells cannot have the same color.
- Agents receive a penalty (-1) for conflicts violating the constraint and a reward (+1) for satisfying it.
- Additional reward (-x) added to total payoff as the average of certain factors.
- Each factor is assigned a value between 0 and 1 for both human and robot agents (e.g. h and r).
- Values are summed for each factor and processed to obtain the additional reward (-x): f(h,r) = -EXP(SUM(h,r)) = -x.

## Q-Tables (used in training) vs Payoff Tables (used in α-rank)
**Q-Table Key:** agent ~ [(cell0, col), (cell1, col), (cell2, col), …]
- where agent is either human or robot.
- where col can be either a color or None.
- the value of this key indicates how good this solution proposed by the agent is.
- this corresponds to a strategy of an agent, e.g., M.

**Payoff Table Key:** {human, robot} ~ {[(cell0, col), (cell1, col), (cell2, col), …], [(cell0, col), (cell1, col), (cell2, col), …]}
- where col can be either a color or None.
- the value of this key indicates how good this solution collectively proposed by both agents is.
- this corresponds to a combination of strategies of the agents, e.g., (M, N).
- the payoff value of that combination (M, N) is defined by the q-values of the two agents: {q_human(M), q_robot(N)}.

---
## NOTES:
1) Q-Learning: Ως state θεωρούμε κάθε διαφορετικό cell. Ως action ορίζουμε την επιλογή color στο εκάστοτε state. Άρα πρόκειται για 1-state αλγόριθμο με Q(s,a) = Q(cell, color). Υπάρχουν συνολικά num_cells * num_colors actions --μικρός αριθμός τουτέστιν 25*5=125.
2) Σε κάθε round (exploration & exploitation) το περιβάλλον αναθέτει ένα cell randomly σε έναν από τους δύο players.
3) Στο exploration η επιλογή action γίνεται randomly μεταξύ των διαθέσιμων actions, ενώ στο exploitation επιλέγεται το maximum Q(cell, color) δεδομένου του cell (state).
4) Στο τέλος κάθε round προκύπτει μια λύση (καλή ή κακή) που αντιστοιχεί σε ένα profile [(ag, cell, col), (ag, cell, col), ... (ag, cell, col)].
5) Αν θέλω να προσθέσω μια λύση στο payoff_tables χρειάζεται να υπολογίσω το payoff value της: P = [w * Q_ag(cell, col) + w * Q_ag(cell, col) + ... + w * Q_ag(cell, col)] / num_cells.
6) Το evaluation γίνεται με 1 population. Άρα το payoff_tables περιλαμβάνει num_populations = 1 table διαστάσεων num_profiles x num_profiles. Οι τιμές σε κάθε row είναι όλες ίσες με το P του τρέχοντος profile. Εφόσον ο στόχος είναι να κάνουμε rank διάφορες λύσεις μεταξύ τους, πρέπει αυτές να διατηρούν ένα σταθερό P value across all columns, που υποδηλώνει το πόσο υπερτερούν ή υστερούν έναντι όλων των άλλων λύσεων.

---
## TODOs:

1) Φτιάχνω το player.py, ορίζω super class Agent με subclasses Human και Robot.
2) Υλοποιώ ενδεικτικά 10 stages, με τους πράκτορες να παίζουν randomly.
3) Μελετώ το qlearning.py του open_spiel.
4) Αν δεν προσαρμόζεται στον κώδικα, ορίζω δικό μου qlearning.
5) Ολοκληρώνω με το evaluation κομμάτι: προσθέτω τον κώδικα στο fork και τρέχω τον α-rank (στο vm).
